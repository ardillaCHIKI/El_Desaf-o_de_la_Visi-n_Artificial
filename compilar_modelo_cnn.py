# Issue 1(F3): CompilaciÃ³n del Modelo
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Input

print("=" * 70)
print("ISSUE 1(F3): COMPILACIÃ“N DEL MODELO")
print("=" * 70)

# ========== CONSTRUIR EL MODELO COMPLETO ==========
print("\nğŸ“¦ Construyendo el modelo CNN completo...")

model = Sequential(name="CNN_CIFAR10")

# Input
model.add(Input(shape=(32, 32, 3)))

# Bloques convolucionales
model.add(Conv2D(32, (3, 3), activation='relu', name='conv2d_1'))
model.add(MaxPooling2D((2, 2), name='maxpool_1'))
model.add(Conv2D(64, (3, 3), activation='relu', name='conv2d_2'))
model.add(MaxPooling2D((2, 2), name='maxpool_2'))

# Capas densas
model.add(Flatten(name='flatten'))
model.add(Dense(64, activation='relu', name='dense_hidden'))
model.add(Dense(10, activation='softmax', name='dense_output'))

print("âœ… Modelo construido correctamente\n")

# ========== COMPILAR EL MODELO ==========
print("=" * 70)
print("COMPILANDO EL MODELO:")
print("=" * 70)

print("\nConfigurando parÃ¡metros de compilaciÃ³n:")
print("  â€¢ Optimizer: 'adam'")
print("  â€¢ Loss: 'categorical_crossentropy'")
print("  â€¢ Metrics: ['accuracy']")

model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

print("\nâœ… MODELO COMPILADO EXITOSAMENTE")

# ========== VERIFICAR LA COMPILACIÃ“N ==========
print("\n" + "=" * 70)
print("VERIFICACIÃ“N DE LA COMPILACIÃ“N:")
print("=" * 70)

# Verificar que el modelo estÃ¡ compilado
assert model.optimizer is not None, "âŒ Error: El modelo no tiene optimizador"
assert model.loss is not None, "âŒ Error: El modelo no tiene funciÃ³n de pÃ©rdida"
assert len(model.metrics) > 0, "âŒ Error: El modelo no tiene mÃ©tricas"

print("âœ… Optimizador configurado:", model.optimizer.__class__.__name__)
print("âœ… FunciÃ³n de pÃ©rdida configurada:", model.loss)
print("âœ… MÃ©tricas configuradas:", [m.name for m in model.metrics])

# ========== RESUMEN DEL MODELO COMPILADO ==========
print("\n" + "=" * 70)
print("RESUMEN DEL MODELO COMPILADO:")
print("=" * 70)
model.summary()

# ========== INFORMACIÃ“N DETALLADA DEL OPTIMIZADOR ==========
print("\n" + "=" * 70)
print("INFORMACIÃ“N DETALLADA DEL OPTIMIZADOR ADAM:")
print("=" * 70)

optimizer_config = model.optimizer.get_config()
print(f"""
Optimizador: {model.optimizer.__class__.__name__}

HiperparÃ¡metros por defecto:
  â€¢ Learning rate (Î±): {optimizer_config.get('learning_rate', 0.001)}
  â€¢ Beta_1 (momento): {optimizer_config.get('beta_1', 0.9)}
  â€¢ Beta_2 (RMSprop): {optimizer_config.get('beta_2', 0.999)}
  â€¢ Epsilon: {optimizer_config.get('epsilon', 1e-07)}

Estos valores son Ã³ptimos para la mayorÃ­a de casos.
Se pueden ajustar si es necesario con:
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001)
""")

# ========== EXPLICACIÃ“N DE CADA PARÃMETRO ==========
print("\n" + "=" * 70)
print("ğŸ§  EXPLICACIÃ“N DE CADA PARÃMETRO DE compile():")
print("=" * 70)

print("""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1ï¸âƒ£  OPTIMIZER = 'adam'                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                â”‚
â”‚ Â¿QuÃ© es un optimizador?                                       â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                      â”‚
â”‚ Es el ALGORITMO que ajusta los pesos de la red durante el     â”‚
â”‚ entrenamiento para minimizar la funciÃ³n de pÃ©rdida.           â”‚
â”‚                                                                â”‚
â”‚ FÃ³rmula bÃ¡sica:                                               â”‚
â”‚     peso_nuevo = peso_viejo - learning_rate Ã— gradiente       â”‚
â”‚                                                                â”‚
â”‚ Â¿QuÃ© hace ADAM?                                               â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                               â”‚
â”‚ Adam = Adaptive Moment Estimation                             â”‚
â”‚ â€¢ Ajusta el learning rate automÃ¡ticamente para cada peso     â”‚
â”‚ â€¢ Combina dos tÃ©cnicas:                                       â”‚
â”‚   1. Momentum: Acumula gradientes pasados (acelera)          â”‚
â”‚   2. RMSprop: Escala segÃºn magnitud de gradientes            â”‚
â”‚                                                                â”‚
â”‚ Ventajas de Adam:                                             â”‚
â”‚ âœ… RÃ¡pida convergencia                                        â”‚
â”‚ âœ… Funciona bien con learning rate por defecto (0.001)        â”‚
â”‚ âœ… Robusto ante gradientes ruidosos                           â”‚
â”‚ âœ… Adaptativo: ajusta learning rate por parÃ¡metro            â”‚
â”‚ âœ… Requiere poca o ninguna tunificaciÃ³n                       â”‚
â”‚ âœ… Muy popular en deep learning                               â”‚
â”‚                                                                â”‚
â”‚ ComparaciÃ³n con otros optimizadores:                          â”‚
â”‚                                                                â”‚
â”‚ SGD (Stochastic Gradient Descent):                           â”‚
â”‚   â€¢ Simple pero lento                                         â”‚
â”‚   â€¢ Requiere ajustar learning rate manualmente               â”‚
â”‚   â€¢ Puede quedar atrapado en mÃ­nimos locales                 â”‚
â”‚                                                                â”‚
â”‚ RMSprop:                                                      â”‚
â”‚   â€¢ Mejor que SGD, pero Adam es superior                      â”‚
â”‚   â€¢ No tiene componente de momentum                           â”‚
â”‚                                                                â”‚
â”‚ Adam:                                                         â”‚
â”‚   â€¢ âœ… MEJOR OPCIÃ“N para CNNs                                 â”‚
â”‚   â€¢ Combina ventajas de Momentum + RMSprop                    â”‚
â”‚   â€¢ Converge mÃ¡s rÃ¡pido y de forma mÃ¡s estable               â”‚
â”‚                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2ï¸âƒ£  LOSS = 'categorical_crossentropy'                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                â”‚
â”‚ Â¿QuÃ© es la funciÃ³n de pÃ©rdida (loss)?                        â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                         â”‚
â”‚ Es la funciÃ³n que MIDE quÃ© tan mal estÃ¡ haciendo la red      â”‚
â”‚ sus predicciones. El objetivo del entrenamiento es            â”‚
â”‚ MINIMIZAR esta funciÃ³n.                                       â”‚
â”‚                                                                â”‚
â”‚ Â¿QuÃ© es Categorical Crossentropy?                            â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                            â”‚
â”‚ Es la funciÃ³n de pÃ©rdida ESTÃNDAR para clasificaciÃ³n         â”‚
â”‚ multiclase cuando las etiquetas estÃ¡n en formato one-hot.     â”‚
â”‚                                                                â”‚
â”‚ FÃ³rmula:                                                      â”‚
â”‚     Loss = -Î£(y_true Ã— log(y_pred))                          â”‚
â”‚                                                                â”‚
â”‚ Ejemplo prÃ¡ctico:                                             â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                             â”‚
â”‚ Imagen real: "gato" (clase 3)                                â”‚
â”‚                                                                â”‚
â”‚ Etiqueta real (one-hot):                                      â”‚
â”‚     [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]                           â”‚
â”‚              â†‘                                                â”‚
â”‚           clase 3                                             â”‚
â”‚                                                                â”‚
â”‚ PredicciÃ³n de la red:                                         â”‚
â”‚     [0.05, 0.03, 0.10, 0.65, 0.02, 0.05, 0.03, 0.02, 0.03, 0.02] â”‚
â”‚                         â†‘                                      â”‚
â”‚                    65% confianza                              â”‚
â”‚                                                                â”‚
â”‚ CÃ¡lculo de pÃ©rdida:                                           â”‚
â”‚     Loss = -log(0.65) = 0.43                                  â”‚
â”‚                                                                â”‚
â”‚ Si la predicciÃ³n fuera perfecta (1.0 para clase 3):          â”‚
â”‚     Loss = -log(1.0) = 0  â† Â¡PERFECTO!                       â”‚
â”‚                                                                â”‚
â”‚ Si la predicciÃ³n fuera mala (0.01 para clase 3):             â”‚
â”‚     Loss = -log(0.01) = 4.6  â† Â¡MUY MAL!                     â”‚
â”‚                                                                â”‚
â”‚ Â¿Por quÃ© usar Categorical Crossentropy?                      â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                      â”‚
â”‚ âœ… DiseÃ±ada especÃ­ficamente para clasificaciÃ³n multiclase     â”‚
â”‚ âœ… Penaliza fuertemente predicciones incorrectas              â”‚
â”‚ âœ… Funciona perfectamente con softmax en la Ãºltima capa       â”‚
â”‚ âœ… Gradientes bien comportados (facilita entrenamiento)       â”‚
â”‚ âœ… InterpretaciÃ³n probabilÃ­stica clara                        â”‚
â”‚                                                                â”‚
â”‚ Alternativas (NO usar aquÃ­):                                  â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                      â”‚
â”‚ â€¢ binary_crossentropy: Solo para 2 clases                    â”‚
â”‚ â€¢ sparse_categorical_crossentropy: Etiquetas como enteros    â”‚
â”‚   (usarÃ­amos esta si y_train fuera [0,1,2,...,9] en vez      â”‚
â”‚    de one-hot)                                                â”‚
â”‚ â€¢ MSE (Mean Squared Error): Para regresiÃ³n, NO clasificaciÃ³n â”‚
â”‚                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3ï¸âƒ£  METRICS = ['accuracy']                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                â”‚
â”‚ Â¿QuÃ© son las mÃ©tricas?                                        â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                         â”‚
â”‚ Son las medidas que usamos para EVALUAR el rendimiento del   â”‚
â”‚ modelo durante el entrenamiento. A diferencia de la loss,     â”‚
â”‚ las mÃ©tricas son para NOSOTROS (humanos), no para el          â”‚
â”‚ algoritmo de optimizaciÃ³n.                                     â”‚
â”‚                                                                â”‚
â”‚ Â¿QuÃ© es Accuracy (exactitud)?                                â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                â”‚
â”‚ Es el porcentaje de predicciones correctas.                   â”‚
â”‚                                                                â”‚
â”‚ FÃ³rmula:                                                      â”‚
â”‚     Accuracy = (Predicciones correctas) / (Total predicciones) â”‚
â”‚                                                                â”‚
â”‚ Ejemplo:                                                      â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€                                                      â”‚
â”‚ De 100 imÃ¡genes:                                              â”‚
â”‚   â€¢ 85 clasificadas correctamente                            â”‚
â”‚   â€¢ 15 clasificadas incorrectamente                          â”‚
â”‚   â†’ Accuracy = 85/100 = 0.85 = 85%                           â”‚
â”‚                                                                â”‚
â”‚ Â¿Por quÃ© usar Accuracy?                                       â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                       â”‚
â”‚ âœ… FÃ¡cil de interpretar (porcentaje)                          â”‚
â”‚ âœ… Intuitiva: "Â¿CuÃ¡ntas acertÃ©?"                             â”‚
â”‚ âœ… EstÃ¡ndar en clasificaciÃ³n                                  â”‚
â”‚ âœ… Permite comparar modelos fÃ¡cilmente                        â”‚
â”‚                                                                â”‚
â”‚ Diferencia con Loss:                                          â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                          â”‚
â”‚ LOSS:                                                         â”‚
â”‚   â€¢ Para el optimizador (se minimiza)                        â”‚
â”‚   â€¢ Valores continuos (0.43, 1.2, etc.)                      â”‚
â”‚   â€¢ Mide "quÃ© tan equivocadas" estÃ¡n las probabilidades      â”‚
â”‚                                                                â”‚
â”‚ ACCURACY:                                                     â”‚
â”‚   â€¢ Para evaluar humanamente                                  â”‚
â”‚   â€¢ Valores 0-1 (0%-100%)                                     â”‚
â”‚   â€¢ Mide "cuÃ¡ntas" acertamos (binario: bien/mal)             â”‚
â”‚                                                                â”‚
â”‚ Otras mÃ©tricas disponibles:                                   â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                      â”‚
â”‚ â€¢ precision: De las predichas como X, Â¿cuÃ¡ntas eran X?       â”‚
â”‚ â€¢ recall: De las que eran X, Â¿cuÃ¡ntas detectamos?            â”‚
â”‚ â€¢ f1-score: Media armÃ³nica de precision y recall              â”‚
â”‚ â€¢ top_k_accuracy: Â¿EstÃ¡ la clase correcta en top-k?          â”‚
â”‚                                                                â”‚
â”‚ Para CIFAR-10, accuracy es suficiente y estÃ¡ndar.            â”‚
â”‚                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
""")

# ========== POR QUÃ‰ ADAM ES ADECUADO PARA CNNS ==========
print("\n" + "=" * 70)
print("ğŸ¯ Â¿POR QUÃ‰ ADAM ES ADECUADO PARA CNNS?")
print("=" * 70)

print("""
CARACTERÃSTICAS DE LAS CNNS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Muchos parÃ¡metros (150K+ en nuestro modelo)
2. Gradientes con diferentes magnitudes en distintas capas
3. Datos de alta dimensionalidad (imÃ¡genes)
4. Riesgo de gradientes desvanecientes/explosivos

VENTAJAS DE ADAM PARA CNNS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

âœ… 1. ADAPTATIVO POR PARÃMETRO
   â€¢ Cada peso tiene su propio learning rate
   â€¢ Capas profundas convergen tan bien como capas superficiales
   â€¢ Especialmente Ãºtil cuando hay muchos parÃ¡metros

âœ… 2. MANEJO DE GRADIENTES RUIDOSOS
   â€¢ Los mini-batches causan gradientes ruidosos
   â€¢ Adam promedia gradientes (momentum) para suavizar
   â€¢ MÃ¡s estable que SGD simple

âœ… 3. NO REQUIERE AJUSTE DE LEARNING RATE
   â€¢ Learning rate por defecto (0.001) funciona muy bien
   â€¢ Con SGD tendrÃ­as que probar: 0.1, 0.01, 0.001, 0.0001...
   â€¢ Adam "encuentra" el learning rate Ã³ptimo automÃ¡ticamente

âœ… 4. RÃPIDA CONVERGENCIA
   â€¢ Combina momentum (aceleraciÃ³n) + adaptaciÃ³n
   â€¢ Converge en menos Ã©pocas que SGD
   â€¢ Ahorra tiempo de entrenamiento

âœ… 5. ROBUSTO CON DIFERENTES ARQUITECTURAS
   â€¢ Funciona bien sea la red poco o muy profunda
   â€¢ No necesitas cambiar hiperparÃ¡metros al cambiar arquitectura
   â€¢ "Fire and forget" optimizer

EJEMPLO COMPARATIVO:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Entrenar este modelo en CIFAR-10 (50,000 imÃ¡genes):

SGD (learning_rate=0.01):
  â€¢ Ã‰poca 1: Loss=2.1, Accuracy=15%
  â€¢ Ã‰poca 10: Loss=1.5, Accuracy=45%
  â€¢ Ã‰poca 20: Loss=1.2, Accuracy=60%
  â€¢ Ã‰poca 50: Loss=0.8, Accuracy=68%
  â±ï¸  Tiempo: ~45 minutos

Adam (learning_rate=0.001):
  â€¢ Ã‰poca 1: Loss=1.8, Accuracy=35%  â† Ya mejor desde el inicio
  â€¢ Ã‰poca 10: Loss=0.9, Accuracy=65%
  â€¢ Ã‰poca 20: Loss=0.6, Accuracy=75%
  â€¢ Ã‰poca 50: Loss=0.4, Accuracy=82%
  â±ï¸  Tiempo: ~45 minutos
  
â†’ âœ… Adam alcanza MEJOR accuracy en MENOS Ã©pocas

CUÃNDO CONSIDERAR OTROS OPTIMIZADORES:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ SGD + Momentum: Si tienes MUCHO tiempo para tunear hiperparÃ¡metros
                   (puede alcanzar marginalmente mejor accuracy)
â€¢ RMSprop: Si Adam da problemas (raro)
â€¢ AdamW: VersiÃ³n de Adam con mejor regularizaciÃ³n (L2 decay)

Para CIFAR-10 y este modelo: Adam es LA MEJOR ELECCIÃ“N âœ…
""")

# ========== JUSTIFICACIÃ“N DE CATEGORICAL CROSSENTROPY ==========
print("\n" + "=" * 70)
print("ğŸ¯ JUSTIFICACIÃ“N DE CATEGORICAL_CROSSENTROPY:")
print("=" * 70)

print("""
CARACTERÃSTICAS DE NUESTRO PROBLEMA:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ ClasificaciÃ³n MULTICLASE (10 clases)
â€¢ Etiquetas en formato ONE-HOT
â€¢ Una sola clase correcta por imagen
â€¢ Ãšltima capa: softmax (probabilidades que suman 1)

Â¿POR QUÃ‰ CATEGORICAL CROSSENTROPY?
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

âœ… 1. DISEÃ‘ADA PARA CLASIFICACIÃ“N MULTICLASE
   â€¢ Maneja naturalmente mÃºltiples clases
   â€¢ Compatible con one-hot encoding
   â€¢ Penaliza predicciones incorrectas proporcionalmente

âœ… 2. COMPLEMENTA PERFECTAMENTE A SOFTMAX
   â€¢ Softmax convierte logits en probabilidades
   â€¢ Crossentropy mide distancia entre distribuciones
   â€¢ Juntas forman un par matemÃ¡ticamente elegante
   
   FÃ³rmula completa:
   L = -Î£(y_true Ã— log(softmax(z)))
   
   Donde z son los logits (salida pre-softmax)

âœ… 3. INTERPRETACIÃ“N PROBABILÃSTICA
   â€¢ Minimizar crossentropy = maximizar log-likelihood
   â€¢ Equivalente a maximizar P(clase_correcta | imagen)
   â€¢ Fundamento teÃ³rico sÃ³lido (teorÃ­a de informaciÃ³n)

âœ… 4. GRADIENTES BIEN COMPORTADOS
   â€¢ Derivada de crossentropy + softmax es limpia:
     âˆ‚L/âˆ‚z = (y_pred - y_true)
   â€¢ No sufre de saturaciÃ³n de gradientes
   â€¢ Facilita backpropagation

âœ… 5. PENALIZA CONFIANZA INCORRECTA
   
   Ejemplo 1: PredicciÃ³n correcta y confiada
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Real:       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  (gato)
   PredicciÃ³n: [0, 0, 0, 0.95, 0, 0, 0, 0, 0, 0.05]
   Loss: -log(0.95) = 0.05  â† Â¡MUY BAJO! âœ…
   
   Ejemplo 2: PredicciÃ³n correcta pero insegura
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Real:       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  (gato)
   PredicciÃ³n: [0.1, 0.1, 0.1, 0.4, 0.1, 0.1, 0.05, 0, 0, 0.05]
   Loss: -log(0.4) = 0.92  â† MÃ¡s alto (se penaliza inseguridad)
   
   Ejemplo 3: PredicciÃ³n incorrecta
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Real:       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  (gato)
   PredicciÃ³n: [0, 0.8, 0, 0.05, 0, 0, 0.1, 0, 0, 0.05]
                   â†‘ predice "automobile"
   Loss: -log(0.05) = 3.0  â† Â¡MUY ALTO! âŒ PenalizaciÃ³n fuerte

COMPARACIÃ“N CON OTRAS LOSS FUNCTIONS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

âŒ Mean Squared Error (MSE):
   â€¢ Para regresiÃ³n (predecir valores continuos)
   â€¢ NO adecuada para clasificaciÃ³n
   â€¢ Gradientes dÃ©biles cuando error es grande
   â€¢ No tiene interpretaciÃ³n probabilÃ­stica

âŒ Binary Crossentropy:
   â€¢ Solo para 2 clases (binario)
   â€¢ Para CIFAR-10 tenemos 10 clases â†’ NO usar

âœ… Sparse Categorical Crossentropy:
   â€¢ Similar pero para etiquetas como enteros [0,1,2,...,9]
   â€¢ Si NO usÃ¡ramos one-hot, esta serÃ­a la alternativa
   â€¢ Como usamos one-hot â†’ categorical es correcta

âŒ Hinge Loss:
   â€¢ DiseÃ±ada para SVMs
   â€¢ NO es probabilÃ­stica
   â€¢ Menos comÃºn en deep learning

REGLA PRÃCTICA:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ ClasificaciÃ³n binaria (2 clases) â†’ binary_crossentropy
â€¢ ClasificaciÃ³n multiclase + one-hot â†’ categorical_crossentropy âœ…
â€¢ ClasificaciÃ³n multiclase + enteros â†’ sparse_categorical_crossentropy
â€¢ RegresiÃ³n â†’ mse, mae

CONCLUSIÃ“N:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Para CIFAR-10 con one-hot encoding y softmax, 
categorical_crossentropy es la elecciÃ³n ESTÃNDAR y Ã“PTIMA.
""")

# ========== RESUMEN EJECUTIVO ==========
print("\n" + "=" * 70)
print("ğŸ“‹ RESUMEN EJECUTIVO:")
print("=" * 70)

print("""
CONFIGURACIÃ“N DE COMPILACIÃ“N:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

model.compile(
    optimizer='adam',              â† Actualiza pesos eficientemente
    loss='categorical_crossentropy',  â† Mide error de clasificaciÃ³n
    metrics=['accuracy']           â† EvalÃºa rendimiento humanamente
)

JUSTIFICACIONES:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. ADAM:
   âœ… Mejor optimizador para CNNs
   âœ… Adaptativo, rÃ¡pido, robusto
   âœ… No requiere tunear learning rate
   âœ… Converge mÃ¡s rÃ¡pido que SGD

2. CATEGORICAL_CROSSENTROPY:
   âœ… EstÃ¡ndar para clasificaciÃ³n multiclase
   âœ… Compatible con one-hot + softmax
   âœ… InterpretaciÃ³n probabilÃ­stica
   âœ… Gradientes bien comportados

3. ACCURACY:
   âœ… FÃ¡cil de interpretar (porcentaje)
   âœ… MÃ©trica estÃ¡ndar de clasificaciÃ³n
   âœ… Permite comparar modelos

ESTADO DEL MODELO:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ… Arquitectura: Completa (7 capas)
âœ… CompilaciÃ³n: Exitosa
âœ… Listo para: Entrenar con fit()

PRÃ“XIMO PASO:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Entrenar el modelo con:
    model.fit(x_train, y_train, epochs=20, validation_split=0.2)
""")

# ========== VERIFICACIÃ“N FINAL ==========
print("\n" + "=" * 70)
print("VERIFICACIÃ“N FINAL:")
print("=" * 70)

print(f"""
âœ… Modelo compilado correctamente
âœ… Optimizador: {model.optimizer.__class__.__name__}
âœ… FunciÃ³n de pÃ©rdida: {model.loss}
âœ… MÃ©tricas: {[m.name for m in model.metrics]}
âœ… Total de parÃ¡metros: {model.count_params():,}
âœ… Modelo listo para entrenamiento

Estado: COMPILACIÃ“N EXITOSA âœ“
""")

print("=" * 70)
print("ISSUE 1(F3) COMPLETADO âœ…")
print("=" * 70)
print("\nğŸš€ El modelo estÃ¡ listo para entrenar!")
print("ğŸ“Š PrÃ³ximo paso: Cargar datos normalizados y entrenar con fit()")