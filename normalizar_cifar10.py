# Issue 3: NormalizaciÃ³n de ImÃ¡genes
import numpy as np
from tensorflow.keras.datasets import cifar10

print("=" * 70)
print("ISSUE 3: NORMALIZACIÃ“N DE IMÃGENES")
print("=" * 70)

# Cargar el dataset
(x_train, y_train), (x_test, y_test) = cifar10.load_data()

print("\nğŸ“Š ANTES de la normalizaciÃ³n:")
print(f"  â€¢ Forma de x_train: {x_train.shape}")
print(f"  â€¢ Forma de x_test:  {x_test.shape}")
print(f"  â€¢ Tipo de datos: {x_train.dtype}")
print(f"  â€¢ Rango de valores: [{x_train.min()}, {x_train.max()}]")
print(f"  â€¢ Media: {x_train.mean():.2f}")
print(f"  â€¢ DesviaciÃ³n estÃ¡ndar: {x_train.std():.2f}")

# Normalizar los valores de pÃ­xeles (dividir entre 255)
x_train_norm = x_train.astype('float32') / 255.0
x_test_norm = x_test.astype('float32') / 255.0

print("\nğŸ“Š DESPUÃ‰S de la normalizaciÃ³n:")
print(f"  â€¢ Forma de x_train_norm: {x_train_norm.shape}")
print(f"  â€¢ Forma de x_test_norm:  {x_test_norm.shape}")
print(f"  â€¢ Tipo de datos: {x_train_norm.dtype}")
print(f"  â€¢ Rango de valores: [{x_train_norm.min()}, {x_train_norm.max()}]")
print(f"  â€¢ Media: {x_train_norm.mean():.4f}")
print(f"  â€¢ DesviaciÃ³n estÃ¡ndar: {x_train_norm.std():.4f}")

# Verificaciones de seguridad
assert x_train_norm.shape == (50000, 32, 32, 3), "âŒ Error: forma de x_train cambiÃ³"
assert x_test_norm.shape == (10000, 32, 32, 3), "âŒ Error: forma de x_test cambiÃ³"
assert 0 <= x_train_norm.min() <= x_train_norm.max() <= 1, "âŒ Error: valores fuera de rango [0,1]"
assert 0 <= x_test_norm.min() <= x_test_norm.max() <= 1, "âŒ Error: valores fuera de rango [0,1]"

print("\nâœ… VerificaciÃ³n: NormalizaciÃ³n correcta")
print("âœ… La forma se mantiene: (32, 32, 3)")
print("âœ… Todos los valores estÃ¡n en el rango [0, 1]")

# Comentario sobre la normalizaciÃ³n
print("\n" + "=" * 70)
print("ğŸ§  Â¿POR QUÃ‰ LA NORMALIZACIÃ“N MEJORA EL ENTRENAMIENTO?")
print("=" * 70)
print("""
1. CONVERGENCIA MÃS RÃPIDA:
   â€¢ Los optimizadores (SGD, Adam) funcionan mejor con valores pequeÃ±os
   â€¢ La red aprende mÃ¡s rÃ¡pido cuando los inputs estÃ¡n en escala similar
   â€¢ Se necesitan menos Ã©pocas para alcanzar buenos resultados

2. ESTABILIDAD NUMÃ‰RICA:
   â€¢ Evita valores muy grandes que pueden causar overflow
   â€¢ Previene problemas de precisiÃ³n en operaciones con float32
   â€¢ Reduce el riesgo de NaN (Not a Number) durante el entrenamiento

3. PREVIENE GRADIENTES EXPLOSIVOS/DESAPARECIDOS:
   â€¢ Valores grandes â†’ gradientes grandes â†’ inestabilidad
   â€¢ Valores pequeÃ±os y normalizados â†’ gradientes controlados
   â€¢ Facilita el flujo de informaciÃ³n durante backpropagation

4. EQUILIBRIO ENTRE FEATURES:
   â€¢ Todos los pÃ­xeles estÃ¡n en la misma escala [0, 1]
   â€¢ NingÃºn pÃ­xel domina el aprendizaje por tener valores mayores
   â€¢ La red trata todas las features con igual importancia inicial

5. COMPATIBILIDAD CON FUNCIONES DE ACTIVACIÃ“N:
   â€¢ Sigmoid y Tanh saturan con valores grandes (gradiente â†’ 0)
   â€¢ ReLU funciona mejor con inputs normalizados
   â€¢ Mejora la efectividad de las activaciones

6. MEJOR INICIALIZACIÃ“N DE PESOS:
   â€¢ Los pesos iniciales (Xavier, He) se diseÃ±an para inputs normalizados
   â€¢ La inicializaciÃ³n funciona Ã³ptimamente con valores en [0, 1] o [-1, 1]
   â€¢ Reduce el tiempo de "warm-up" del entrenamiento

7. AJUSTE MÃS FÃCIL DE LEARNING RATE:
   â€¢ Con inputs normalizados, el learning rate es mÃ¡s intuitivo
   â€¢ No es necesario ajustar tanto el learning rate
   â€¢ Mayor estabilidad en el proceso de optimizaciÃ³n
""")

print("\n" + "=" * 70)
print("ğŸ“ˆ IMPACTO PRÃCTICO:")
print("=" * 70)
print("""
SIN normalizaciÃ³n (valores 0-255):
  â€¢ Learning rate tÃ­pico: 0.0001 - 0.00001
  â€¢ Ã‰pocas para converger: 50-100+
  â€¢ Riesgo de inestabilidad: ALTO

CON normalizaciÃ³n (valores 0-1):
  â€¢ Learning rate tÃ­pico: 0.001 - 0.01
  â€¢ Ã‰pocas para converger: 20-50
  â€¢ Riesgo de inestabilidad: BAJO
""")

print("=" * 70)
print("ISSUE 3 COMPLETADO âœ…")
print("=" * 70)